{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from time import sleep\n",
    "from random import sample\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nactions =  2\n",
    "nparams = 4\n",
    "learning_rate = 0.05\n",
    "gamma = .3\n",
    "\n",
    "nepisode = 50\n",
    "\n",
    "nbatch = 100\n",
    "batch_legth = 512\n",
    "batch_iter = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = tf.placeholder(shape=[1, nparams], dtype=tf.float32)\n",
    "\n",
    "# Hidden layer 1\n",
    "weight = tf.Variable(tf.random_uniform([nparams, nactions],0 , 0.01))\n",
    "bias = tf.Variable(tf.random_uniform([1, nactions], 0, 0.01))\n",
    "Q = tf.add(tf.matmul(_input, weight), bias)\n",
    "Q = tf.sigmoid(Q)\n",
    "\n",
    "Qa = tf.argmax(Q, 1)\n",
    "\n",
    "# weight_1 = tf.Variable(tf.random_uniform([nparams, 10],0 , 0.01))\n",
    "# bias_1 = tf.Variable(tf.random_uniform([1, 10], 0, 0.01))\n",
    "# H1 = tf.add(tf.matmul(_input, weight_1), bias_1)\n",
    "# H1 = tf.tanh(H1)\n",
    "\n",
    "# weight_2 = tf.Variable(tf.random_uniform([10, nactions],0 , 0.01))\n",
    "# bias_2 = tf.Variable(tf.random_uniform([1, nactions], 0, 0.01))\n",
    "# Q = tf.add(tf.matmul(H1, weight_2), bias_2)\n",
    "# Q = tf.tanh(Q)\n",
    "# action = tf.argmax(Q, 1)\n",
    "\n",
    "Qe = tf.placeholder(shape=[1, nactions], dtype=tf.float32)\n",
    "error = tf.reduce_sum(tf.square(Qe - Q))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "beta = 0.5\n",
    "init = tf.global_variables_initializer()\n",
    "steps = list()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for episode in range(nepisode):\n",
    "        print()\n",
    "        # training: Collect play experience\n",
    "        memos = list()\n",
    "        done = False\n",
    "        while len(memos) < batch_legth*batch_iter:\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            while not done:\n",
    "                rand_val = np.random.rand(1)\n",
    "                res_action, res_Q = sess.run([Qa, Q], feed_dict={_input:[state]})\n",
    "                new_state, reward, done, _ = env.step(res_action[0])\n",
    "                memos.append( (state, res_action, reward, new_state, done) )\n",
    "        print(\"\\tCollect Step in Experience: \", len(memos))\n",
    "        \n",
    "        # training: Review playing experience\n",
    "        for iteration in range(batch_iter):\n",
    "            batch = sample(memos, batch_legth)\n",
    "            for state, res_action, reward, new_state, done in batch:\n",
    "                r = reward\n",
    "                if not done:\n",
    "                    next_Q = sess.run(Q, feed_dict={_input:[new_state]})\n",
    "                    r = r + gamma*np.max(next_Q)\n",
    "                else:\n",
    "                    r = -100\n",
    "                res_Q = sess.run(Q, feed_dict={_input:[state]})\n",
    "                res_Q[0, res_action[0]] = r\n",
    "                sess.run([trainer], feed_dict={_input:[state], Qe:res_Q})\n",
    "            \n",
    "                \n",
    "    \n",
    "        # playing\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = sess.run([Qa], feed_dict={_input:[state]})\n",
    "            action = action[0][0]\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            step += 1\n",
    "#         print(step)\n",
    "        steps.append(step)\n",
    "        print(\"in Episode {} agent can live in {} step\".format(episode, step))\n",
    "plt.plot(steps)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "\n",
    "# beta = 0.5\n",
    "\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# steps = [0]\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     for i in range(100):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         step = 0\n",
    "#         r = 0\n",
    "#         print()\n",
    "#         print(i)\n",
    "#         print()\n",
    "#         while done == False:\n",
    "#             env.render()\n",
    "            \n",
    "#             rand_val = np.random.rand(1)\n",
    "#             res_action, res_Q = sess.run([action, Q], feed_dict={_input:[state]})\n",
    "#             # agent do something new\n",
    "#             if  rand_val < beta:\n",
    "#                 new_action = env.action_space.sample()\n",
    "#                 res_action[0] = new_action\n",
    "                \n",
    "#             # do action \n",
    "#             new_state, reward, done, _ = env.step(res_action[0])\n",
    "            \n",
    "#             # cal reward\n",
    "#             if done:\n",
    "# #                 r = -100\n",
    "#                 if step > max(steps):\n",
    "#                     beta = beta*0.8\n",
    "#                 else:\n",
    "#                     r = -100\n",
    "                \n",
    "# #                 sleep()\n",
    "# #                 break\n",
    "#             else:\n",
    "# #                 r = 0 + (1 if step > max(steps) else 0)\n",
    "#                 r = step\n",
    "#                 step += 1\n",
    "            \n",
    "#             # update weight\n",
    "#             next_Q = sess.run(Q, feed_dict={_input:[new_state]})\n",
    "#             res_Q[0, res_action[0]] = r + gamma*np.max(next_Q)\n",
    "            \n",
    "#             sess.run([trainer], feed_dict={_input:[state], Qe:res_Q})\n",
    "#             print(step, res_action, res_Q, state, end=' ')\n",
    "#             print('.' if rand_val < beta else '', end='')\n",
    "#             print()\n",
    "            \n",
    "#             state = new_state\n",
    "#         steps.append(step)\n",
    "    \n",
    "# plt.plot(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[  8.58846959e-03  -9.89045802e-05]]\n",
    "np.argmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-py3",
   "language": "python",
   "name": "tf-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
