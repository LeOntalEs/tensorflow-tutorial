{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('CartPole-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nepisode = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "#         with tf.variable_scope('network'):\n",
    "        self.input_state = tf.placeholder(shape=[None, 4], dtype=tf.float32)\n",
    "        self.weight_1 = tf.Variable(tf.random_uniform([4, 10], dtype=tf.float32))\n",
    "        self.bias_1 =  tf.Variable(tf.random_uniform([1, 10], dtype=tf.float32))\n",
    "        self.hidden_1 = tf.nn.tanh(tf.add(tf.matmul(self.input_state, self.weight_1), self.bias_1))\n",
    "\n",
    "        self.weight_2 = tf.Variable(tf.random_uniform([10, 2], dtype=tf.float32))\n",
    "        self.bias_2 =  tf.Variable(tf.random_uniform([1, 2], dtype=tf.float32))\n",
    "        self.pred_Q = tf.add(tf.matmul(self.hidden_1, self.weight_2), self.bias_2)\n",
    "        self.select_action = tf.argmax(self.pred_Q, 1)\n",
    "\n",
    "        self.expect_Q = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "#         self.error = tf.reduce_mean(tf.square(self.expect_Q - self.pred_Q))\n",
    "        self.loss = tf.losses.mean_squared_error(self.expect_Q, self.pred_Q)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "    \n",
    "    def predict(self, sess, state):\n",
    "        select_action, pred_Q = sess.run([self.select_action, self.pred_Q], \\\n",
    "                                         feed_dict={self.input_state:state})\n",
    "        return select_action, pred_Q\n",
    "    \n",
    "    def update(self, sess, X, Y):\n",
    "        loss, _ = sess.run([self.loss, self.trainer], feed_dict={self.input_state:X, self.expect_Q:Y})\n",
    "        return loss\n",
    "        \n",
    "class Agent:\n",
    "    def __init__(self, session, beta=0.9, discount=0.3, batch_iter=32, batch_size=16, max_memory=1000):\n",
    "        self.sess = session\n",
    "        self.net = Network()\n",
    "        self.beta = beta\n",
    "        self.experience_memory = list()\n",
    "        self.max_memory = max_memory\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_iter = batch_iter\n",
    "        self.discount = discount\n",
    "        \n",
    "    # state, action, reward, next_state, done\n",
    "    def memory(self, pack):\n",
    "        if len(self.experience_memory) > self.max_memory:\n",
    "            self.experience_memory.pop(0)\n",
    "        self.experience_memory.append( pack )\n",
    "    \n",
    "    def predict(self, state):\n",
    "        is_random = False\n",
    "        select_action, pred_Q = self.net.predict(self.sess, state)\n",
    "        if np.random.rand(1) < self.beta:\n",
    "            is_random = True\n",
    "            select_action = [env.action_space.sample()]\n",
    "        return select_action, pred_Q, is_random\n",
    "    \n",
    "    \n",
    "    def replay_experience(self):\n",
    "        losses = list()\n",
    "        for batchiter in range(self.batch_iter):\n",
    "            X = list()\n",
    "            Y = list()\n",
    "            batch = random.sample(self.experience_memory, \\\n",
    "                min(len(self.experience_memory), self.batch_size))\n",
    "\n",
    "            for state, action, reward, next_state, done in batch:\n",
    "                pq = self.predict([state])[1]\n",
    "                if done:\n",
    "                    pq[0, action] = reward\n",
    "                else:\n",
    "                    pq[0, action] = reward + self.discount * np.max(self.predict([next_state])[1])\n",
    "                X.append(state)\n",
    "                Y.append(pq[0])\n",
    "            loss = self.net.update(sess, np.array(X), np.array(Y))\n",
    "            losses.append( loss )\n",
    "        return np.mean(losses)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in Episode   10 mean step: 10.27, Agent Replay Experience, get loss: 513.46 \n",
      "in Episode   20 mean step: 182.64, Agent Replay Experience, get loss: 30.24 \n",
      "in Episode   30 mean step: 200.00, Agent Replay Experience, get loss: 20.33 \n",
      "in Episode   40 mean step: 169.27, Agent Replay Experience, get loss: 40.64 \n",
      "in Episode   50 mean step: 46.73, Agent Replay Experience, get loss: 81.38 \n",
      "in Episode   60 mean step: 11.82, Agent Replay Experience, get loss: 91.03 \n",
      "in Episode   70 mean step: 9.55, Agent Replay Experience, get loss: 151.38 \n",
      "in Episode   80 mean step: 9.82, Agent Replay Experience, get loss: 176.87 \n",
      "in Episode   90 mean step: 16.45, Agent Replay Experience, get loss: 202.30 \n",
      "in Episode  100 mean step: 17.09, Agent Replay Experience, get loss: 273.75 \n",
      "in Episode  110 mean step: 16.82, Agent Replay Experience, get loss: 304.31 \n",
      "in Episode  120 mean step: 18.91, Agent Replay Experience, get loss: 254.69 \n",
      "in Episode  130 mean step: 11.18, Agent Replay Experience, get loss: 405.91 \n",
      "in Episode  140 mean step: 10.27, Agent Replay Experience, get loss: 264.23 \n",
      "in Episode  150 mean step: 10.55, Agent Replay Experience, get loss: 393.28 \n",
      "in Episode  160 mean step: 9.09, Agent Replay Experience, get loss: 253.71 \n",
      "in Episode  170 mean step: 9.55, Agent Replay Experience, get loss: 207.48 \n",
      "in Episode  180 mean step: 9.45, Agent Replay Experience, get loss: 436.06 \n",
      "in Episode  190 mean step: 8.82, Agent Replay Experience, get loss: 344.48 \n",
      "in Episode  200 mean step: 9.18, Agent Replay Experience, get loss: 350.17 \n",
      "Bot play game and archive 9 step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "steps = []\n",
    "memory = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    agent = Agent(sess)    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(nepisode+1):\n",
    "        done = False\n",
    "        step = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            action, pred_Q, _ = agent.predict([state])\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            \n",
    "            # Big Penalty to Agent\n",
    "            if done: \n",
    "                reward = -100\n",
    "\n",
    "            agent.memory( (state, action, reward, next_state, done) )\n",
    "            \n",
    "            step += 1\n",
    "            state = next_state\n",
    "        steps.append(step)\n",
    "        agent.beta = agent.beta*0.7    \n",
    "        \n",
    "        if episode%10 == 0 and episode != 0:\n",
    "            loss = agent.replay_experience()\n",
    "            print(\"in Episode {:4d} mean step: {:.2f}, Agent Replay Experience, get loss: {:.2f} \"\\\n",
    "                      .format(episode, np.mean(steps[episode-10:episode+1]), loss))\n",
    "\n",
    "    \n",
    "    step = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action, Q, _ = agent.predict([state])\n",
    "        state, reward, done, _ = env.step(action[0])\n",
    "        step += 1\n",
    "        env.render()\n",
    "        sleep(0.1)\n",
    "    print('Bot play game and archive {} step'.format(step))\n",
    "    env.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-py3",
   "language": "python",
   "name": "tf-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
