{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.reset()\n",
    "# _input = tf.placeholder(shape=[None, 128], dtype=tf.float32)\n",
    "# conv1d = tf.nn.conv1d([_input], [1,1], stride=32,padding='SAME')\n",
    "print(state.shape)\n",
    "env.render()\n",
    "# with tf.Session as sess:\n",
    "#     x = tf.run([conv1d], feed_dict({\"_input\":state}))\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "                \n",
    "        self.imw, self.imh, self.imc = (210, 160, 3)\n",
    "        self.input_state = tf.placeholder(tf.float32, shape=[None, self.imw * self.imh * self.imc])\n",
    "        self.x_image = tf.reshape(self.input_state, [-1, self.imw, self.imh, self.imc])\n",
    "\n",
    "        self.conv1 = tf.layers.conv2d(self.x_image, 16, 5, activation=tf.nn.relu)\n",
    "        self.max1 = tf.layers.max_pooling2d(self.conv1, 2, 2)\n",
    "\n",
    "        self.conv2 = tf.layers.conv2d(self.max1, 8, 5, activation=tf.nn.relu)\n",
    "        self.max2 = tf.layers.max_pooling2d(self.conv2, 2, 2)\n",
    "\n",
    "        self.conv3 = tf.layers.conv2d(self.max2, 4, 5, activation=tf.nn.relu)\n",
    "        self.max3 = tf.layers.max_pooling2d(self.conv3, 2, 2)\n",
    "\n",
    "        self.flattern = tf.contrib.layers.flatten(self.max3)\n",
    "        self.fc1 = tf.contrib.layers.fully_connected(self.flattern, 700)\n",
    "        self.fc2 = tf.contrib.layers.fully_connected(self.fc1, 350)\n",
    "        self.fc3 = tf.contrib.layers.fully_connected(self.fc2, 120)\n",
    "        self.fc4 = tf.contrib.layers.fully_connected(self.fc3, 60)\n",
    "        self.fc5 = tf.contrib.layers.fully_connected(self.fc4, 10)\n",
    "        self.pred_Q = tf.contrib.layers.fully_connected(self.fc5, 4)\n",
    "        self.select_action = tf.argmax(self.pred_Q, 1)\n",
    "        \n",
    "        self.expect_Q = tf.placeholder(shape=[None, 4], dtype=tf.float32)\n",
    "        self.loss = tf.losses.mean_squared_error(self.expect_Q, self.pred_Q)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(self.loss)\n",
    "        \n",
    "    def predict(self, sess, state):\n",
    "        select_action, pred_Q = sess.run([self.select_action, self.pred_Q], \\\n",
    "                                         feed_dict={self.input_state:state})\n",
    "        return select_action, pred_Q\n",
    "    \n",
    "    def update(self, sess, X, Y):\n",
    "        loss, _ = sess.run([self.loss, self.trainer], feed_dict={self.input_state:X, self.expect_Q:Y})\n",
    "        return loss\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, beta=0.9, discount=0.3, batch_iter=32, batch_size=16, max_memory=1000):\n",
    "        self.sess = tf.Session()\n",
    "        self.net = Network()\n",
    "        self.beta = beta\n",
    "        self.experience_memory = list()\n",
    "        self.max_memory = max_memory\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_iter = batch_iter\n",
    "        self.discount = discount\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.sess.close()\n",
    "    \n",
    "    def initvar(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def memory(self, pack):\n",
    "        if len(self.experience_memory) > self.max_memory:\n",
    "            self.experience_memory.pop(0)\n",
    "        self.experience_memory.append( pack )\n",
    "    \n",
    "    def predict(self, state):\n",
    "        is_random = False\n",
    "        select_action, pred_Q = self.net.predict(self.sess, state)\n",
    "        if np.random.rand(1) < self.beta:\n",
    "            is_random = True\n",
    "            select_action = [env.action_space.sample()]\n",
    "        return select_action, pred_Q, is_random\n",
    "    \n",
    "    \n",
    "    def replay_experience(self):\n",
    "        losses = list()\n",
    "        for batchiter in range(self.batch_iter):\n",
    "            X = list()\n",
    "            Y = list()\n",
    "            batch = random.sample(self.experience_memory, \\\n",
    "                min(len(self.experience_memory), self.batch_size))\n",
    "\n",
    "            for state, action, reward, next_state, done in batch:\n",
    "                pq = self.predict([state])[1]\n",
    "                if done:\n",
    "                    pq[0, action] = reward\n",
    "                else:\n",
    "                    pq[0, action] = reward + self.discount * np.max(self.predict([next_state])[1])\n",
    "                X.append(state)\n",
    "                Y.append(pq[0])\n",
    "            loss = self.net.update(sess, np.array(X), np.array(Y))\n",
    "            losses.append( loss )\n",
    "        return np.mean(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepisode = 1\n",
    "\n",
    "with Agent() as agent:\n",
    "    agent.initvar()\n",
    "    for episode in range(nepisode+1):\n",
    "        done = False\n",
    "        step = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            action, pred_Q, _ = agent.predict([state])\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            \n",
    "            # Big Penalty to Agent\n",
    "#             if done: \n",
    "#                 reward = -100\n",
    "\n",
    "            agent.memory( (state, action, reward, next_state, done) )\n",
    "            \n",
    "            step += 1\n",
    "            state = next_state\n",
    "        steps.append(step)\n",
    "        agent.beta = agent.beta*0.7    \n",
    "        \n",
    "        if episode%10 == 0 and episode != 0:\n",
    "            loss = agent.replay_experience()\n",
    "            print(\"in Episode {:4d} mean step: {:.2f}, Agent Replay Experience, get loss: {:.2f} \"\\\n",
    "                      .format(episode, np.mean(steps[episode-10:episode+1]), loss))\n",
    "\n",
    "    \n",
    "    step = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action, Q, _ = agent.predict([state])\n",
    "        state, reward, done, _ = env.step(action[0])\n",
    "        step += 1\n",
    "        env.render()\n",
    "        sleep(0.1)\n",
    "    print('Bot play game and archive {} step'.format(step))\n",
    "    env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-py3",
   "language": "python",
   "name": "tf-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
